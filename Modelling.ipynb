{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Classification Models Analysis\n",
    "\n",
    "## Models Implemented\n",
    "1. Deep Learning (BERT)\n",
    "2. Shallow Learning (SVM)\n",
    "\n",
    "## Why These Models Were Chosen\n",
    "\n",
    "### BERT Model\n",
    "- Selected for its ability to handle hierarchical text classification\n",
    "- Leverages pre-trained language understanding capabilities\n",
    "- Can capture complex semantic relationships in text\n",
    "- Implements a shared feature layer architecture to learn common representations across hierarchy levels\n",
    "\n",
    "### SVM Model\n",
    "- Chosen as a baseline traditional machine learning approach\n",
    "- Computationally less intensive than deep learning\n",
    "- Good for high-dimensional sparse data (text classification)\n",
    "- Uses hierarchical structure with separate classifiers for each level\n",
    "\n",
    "## Model Architecture & Features\n",
    "\n",
    "### BERT Implementation\n",
    "- Uses bert-base-uncased as base model\n",
    "- Hierarchical structure with 3 classification levels\n",
    "- Shared layer architecture for feature learning\n",
    "- Dropout (0.3) for regularization\n",
    "- AdamW optimizer with learning rate 2e-5\n",
    "\n",
    "### SVM Implementation\n",
    "- TF-IDF vectorization with 10000 features\n",
    "- Linear SVM with balanced class weights\n",
    "- Hierarchical structure with separate classifiers per level\n",
    "- Handles single-class cases specially\n",
    "\n",
    "## Performance Results\n",
    "\n",
    "### BERT Model Results (After 10 epochs)\n",
    "| Metric | Level 1 | Level 2 | Level 3 |\n",
    "|--------|---------|---------|---------|\n",
    "| Accuracy | 93.15% | 83.95% | 75.40% |\n",
    "| F1-Score | 93.14% | 83.71% | 73.58% |\n",
    "\n",
    "### SVM Model Results\n",
    "| Metric | Level 1 | Level 2 | Level 3 |\n",
    "|--------|---------|---------|---------|\n",
    "| Accuracy | 88.90% | 73.65% | 65.65% |\n",
    "| F1-Score | 89.00% | 73.00% | 64.00% |\n",
    "\n",
    "### Comparative Analysis\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[Model Performance] --> B[BERT]\n",
    "    A --> C[SVM]\n",
    "    B --> D[Level 1: 93.15%]\n",
    "    B --> E[Level 2: 83.95%]\n",
    "    B --> F[Level 3: 75.40%]\n",
    "    C --> G[Level 1: 88.90%]\n",
    "    C --> H[Level 2: 73.65%]\n",
    "    C --> I[Level 3: 65.65%]\n",
    "\n",
    "\n",
    "## Key Findings\n",
    "1. BERT outperforms SVM across all hierarchical levels\n",
    "2. Both models show decreasing performance as hierarchy depth increases\n",
    "3. BERT shows better generalization and handling of complex relationships\n",
    "4. SVM provides decent performance with lower computational requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('/Users/nbhagat/hierarchy classification/cleaned_categories_improved.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Leaning\n",
    "### Model: Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalDataset(Dataset):\n",
    "    def __init__(self, texts, labels1, labels2, labels3, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels1 = labels1\n",
    "        self.labels2 = labels2\n",
    "        self.labels3 = labels3\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label1': torch.tensor(self.labels1[idx], dtype=torch.long),\n",
    "            'label2': torch.tensor(self.labels2[idx], dtype=torch.long),\n",
    "            'label3': torch.tensor(self.labels3[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalBertClassifier(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased', num_labels1=6, num_labels2=64, num_labels3=377):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        self.shared_layer = nn.Linear(768, 768)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        self.classifier1 = nn.Linear(768, num_labels1)\n",
    "        self.classifier2 = nn.Linear(768, num_labels2)\n",
    "        self.classifier3 = nn.Linear(768, num_labels3)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        shared_features = self.activation(self.shared_layer(pooled_output))\n",
    "        shared_features = self.dropout(shared_features)\n",
    "        \n",
    "        level1_output = self.classifier1(shared_features)\n",
    "        level2_output = self.classifier2(shared_features)\n",
    "        level3_output = self.classifier3(shared_features)\n",
    "        \n",
    "        return level1_output, level2_output, level3_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalTrainer:\n",
    "    def __init__(self, model, learning_rate=2e-5):\n",
    "        self.device = torch.device('cpu')  \n",
    "        self.model = model.to(self.device)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "\n",
    "    def prepare_data(self, df, max_length=512, batch_size=8):  # Reduced batch size for CPU\n",
    "        # Prepare texts and labels\n",
    "        texts = df['text_combined'].values\n",
    "        labels1 = df['Cat1_encoded'].values\n",
    "        labels2 = df['Cat2_encoded'].values\n",
    "        labels3 = df['Cat3_encoded'].values\n",
    "        \n",
    "        # Split data\n",
    "        train_texts, val_texts, train_l1, val_l1, train_l2, val_l2, train_l3, val_l3 = train_test_split(\n",
    "            texts, labels1, labels2, labels3, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        train_dataset = HierarchicalDataset(\n",
    "            train_texts, train_l1, train_l2, train_l3, \n",
    "            self.tokenizer, max_length\n",
    "        )\n",
    "        val_dataset = HierarchicalDataset(\n",
    "            val_texts, val_l1, val_l2, val_l3, \n",
    "            self.tokenizer, max_length\n",
    "        )\n",
    "        \n",
    "        # Create dataloaders with smaller batch size.\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=batch_size, \n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "\n",
    "    \n",
    "    def train_epoch(self, train_loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "\n",
    "            input_ids = batch['input_ids'].to(self.device)\n",
    "            attention_mask = batch['attention_mask'].to(self.device)\n",
    "            labels1 = batch['label1'].to(self.device)\n",
    "            labels2 = batch['label2'].to(self.device)\n",
    "            labels3 = batch['label3'].to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs1, outputs2, outputs3 = self.model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            # Calculate losses\n",
    "            loss1 = self.criterion(outputs1, labels1)\n",
    "            loss2 = self.criterion(outputs2, labels2)\n",
    "            loss3 = self.criterion(outputs3, labels3)\n",
    "            \n",
    "            loss = loss1 + loss2 + loss3\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        return total_loss / len(train_loader)\n",
    "    \n",
    "    def evaluate(self, val_loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        predictions = []\n",
    "        actuals = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels1 = batch['label1'].to(self.device)\n",
    "                labels2 = batch['label2'].to(self.device)\n",
    "                labels3 = batch['label3'].to(self.device)\n",
    "                \n",
    "                outputs1, outputs2, outputs3 = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask\n",
    "                )\n",
    "                \n",
    "                # Calculate losses\n",
    "                loss1 = self.criterion(outputs1, labels1)\n",
    "                loss2 = self.criterion(outputs2, labels2)\n",
    "                loss3 = self.criterion(outputs3, labels3)\n",
    "                loss = loss1 + loss2 + loss3\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Get predictions\n",
    "                preds1 = torch.argmax(outputs1, dim=1).cpu().numpy()\n",
    "                preds2 = torch.argmax(outputs2, dim=1).cpu().numpy()\n",
    "                preds3 = torch.argmax(outputs3, dim=1).cpu().numpy()\n",
    "                \n",
    "                # Store predictions and actuals\n",
    "                predictions.extend(zip(preds1, preds2, preds3))\n",
    "                actuals.extend(zip(\n",
    "                    labels1.cpu().numpy(),\n",
    "                    labels2.cpu().numpy(),\n",
    "                    labels3.cpu().numpy()\n",
    "                ))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = self.calculate_metrics(predictions, actuals)\n",
    "        metrics['loss'] = total_loss / len(val_loader)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def calculate_metrics(self, predictions, actuals):\n",
    "        # Separate predictions and actuals by level\n",
    "        preds1, preds2, preds3 = zip(*predictions)\n",
    "        acts1, acts2, acts3 = zip(*actuals)\n",
    "        \n",
    "\n",
    "        metrics = {\n",
    "            'level1_accuracy': accuracy_score(acts1, preds1),\n",
    "            'level2_accuracy': accuracy_score(acts2, preds2),\n",
    "            'level3_accuracy': accuracy_score(acts3, preds3),\n",
    "            'level1_f1': f1_score(acts1, preds1, average='weighted'),\n",
    "            'level2_f1': f1_score(acts2, preds2, average='weighted'),\n",
    "            'level3_f1': f1_score(acts3, preds3, average='weighted'),\n",
    "        }\n",
    "        \n",
    "        exact_matches = sum(1 for p, a in zip(predictions, actuals) if p == a)\n",
    "        metrics['exact_match'] = exact_matches / len(predictions)\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def train(self, train_loader, val_loader, epochs=10, early_stopping_patience=3):\n",
    "        best_val_loss = float('inf')\n",
    "        early_stopping_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "            \n",
    "            # Training\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            val_metrics = self.evaluate(val_loader)\n",
    "            \n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_metrics['loss']:.4f}\")\n",
    "            print(f\"Val Exact Match: {val_metrics['exact_match']:.4f}\")\n",
    "            print(f\"Val Level 1 Accuracy: {val_metrics['level1_accuracy']:.4f}\")\n",
    "            print(f\"Val Level 2 Accuracy: {val_metrics['level2_accuracy']:.4f}\")\n",
    "            print(f\"Val Level 3 Accuracy: {val_metrics['level3_accuracy']:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_metrics['loss'] < best_val_loss:\n",
    "                best_val_loss = val_metrics['loss']\n",
    "                early_stopping_counter = 0\n",
    "                torch.save(self.model.state_dict(), 'best_model.pt')\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "            \n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data_for_model(cleaned_df):\n",
    "    cleaned_df['text_combined'] = cleaned_df['Title'] + ' ' + cleaned_df['Text']\n",
    "    le1 = LabelEncoder()\n",
    "    le2 = LabelEncoder()\n",
    "    le3 = LabelEncoder()\n",
    "    \n",
    "    # Fit and transform labels\n",
    "    cleaned_df['Cat1_encoded'] = le1.fit_transform(cleaned_df['Cat1'])\n",
    "    cleaned_df['Cat2_encoded'] = le2.fit_transform(cleaned_df['Cat2'])\n",
    "    cleaned_df['Cat3_encoded'] = le3.fit_transform(cleaned_df['Cat3'])\n",
    "    \n",
    "    encoders = {\n",
    "        'Cat1': le1,\n",
    "        'Cat2': le2,\n",
    "        'Cat3': le3\n",
    "    }\n",
    "    \n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Total samples: {len(cleaned_df)}\")\n",
    "    print(\"\\nUnique categories per level:\")\n",
    "    print(f\"Cat1: {len(le1.classes_)}\")\n",
    "    print(f\"Cat2: {len(le2.classes_)}\")\n",
    "    print(f\"Cat3: {len(le3.classes_)}\")\n",
    "    \n",
    "\n",
    "    model_config = {\n",
    "        'num_labels1': len(le1.classes_),\n",
    "        'num_labels2': len(le2.classes_),\n",
    "        'num_labels3': len(le3.classes_)\n",
    "    }\n",
    "    \n",
    "    return cleaned_df, encoders, model_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cleaned_df, encoders, model_config = prepare_data_for_model(df)\n",
    "model = HierarchicalBertClassifier(\n",
    "    model_name='bert-base-uncased',\n",
    "    num_labels1=model_config['num_labels1'],\n",
    "    num_labels2=model_config['num_labels2'],\n",
    "    num_labels3=model_config['num_labels3']\n",
    ")\n",
    "trainer = HierarchicalTrainer(model)\n",
    "train_loader, val_loader = trainer.prepare_data(cleaned_df)\n",
    "trainer.train(train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores\n",
    "```plaintext\n",
    "F1 Scores:\n",
    "Level 1 F1 Score: 0.9314\n",
    "Level 2 F1 Score: 0.8371\n",
    "Level 3 F1 Score: 0.7358\n",
    "All Metrics:\n",
    "level1_accuracy: 0.9315\n",
    "level2_accuracy: 0.8395\n",
    "level3_accuracy: 0.7540\n",
    "level1_f1: 0.9314\n",
    "level2_f1: 0.8371\n",
    "level3_f1: 0.7358\n",
    "exact_match: 0.7330\n",
    "loss: 2.3248\n",
    "### Training Progress\n",
    "\n",
    "Epoch 1/10\n",
    "Training: 100%|██████████| 500/500 [06:27<00:00,  1.29it/s]\n",
    "Evaluating: 100%|██████████| 125/125 [00:36<00:00,  3.45it/s]\n",
    "Train Loss: 8.2012\n",
    "Val Loss: 6.0403\n",
    "Val Exact Match: 0.2340\n",
    "Val Level 1 Accuracy: 0.8930\n",
    "Val Level 2 Accuracy: 0.5345\n",
    "Val Level 3 Accuracy: 0.2680\n",
    "\n",
    "Epoch 2/10\n",
    "Training: 100%|██████████| 500/500 [06:35<00:00,  1.26it/s]\n",
    "Evaluating: 100%|██████████| 125/125 [00:36<00:00,  3.45it/s]\n",
    "Train Loss: 5.2607\n",
    "Val Loss: 4.5996\n",
    "Val Exact Match: 0.3005\n",
    "Val Level 1 Accuracy: 0.9205\n",
    "Val Level 2 Accuracy: 0.6465\n",
    "Val Level 3 Accuracy: 0.3320\n",
    "\n",
    "Epoch 3/10\n",
    "Training: 100%|██████████| 500/500 [06:33<00:00,  1.27it/s]\n",
    "Evaluating: 100%|██████████| 125/125 [00:35<00:00,  3.55it/s]\n",
    "Train Loss: 3.9247\n",
    "Val Loss: 3.7848\n",
    "Val Exact Match: 0.4075\n",
    "Val Level 1 Accuracy: 0.9280\n",
    "Val Level 2 Accuracy: 0.7125\n",
    "Val Level 3 Accuracy: 0.4485\n",
    "\n",
    "Epoch 4/10\n",
    "Training: 100%|██████████| 500/500 [06:35<00:00,  1.26it/s]\n",
    "Evaluating: 100%|██████████| 125/125 [00:36<00:00,  3.45it/s]\n",
    "Train Loss: 3.0108\n",
    "Val Loss: 3.2671\n",
    "Val Exact Match: 0.5140\n",
    "Val Level 1 Accuracy: 0.9275\n",
    "Val Level 2 Accuracy: 0.7750\n",
    "Val Level 3 Accuracy: 0.5505\n",
    "\n",
    "Epoch 5/10\n",
    "Training: 100%|██████████| 500/500 [06:35<00:00,  1.26it/s]\n",
    "Evaluating: 100%|██████████| 125/125 [00:36<00:00,  3.45it/s]\n",
    "Train Loss: 2.3361\n",
    "Val Loss: 2.9271\n",
    "Val Exact Match: 0.5785\n",
    "Val Level 1 Accuracy: 0.9260\n",
    "Val Level 2 Accuracy: 0.7915\n",
    "Val Level 3 Accuracy: 0.6095\n",
    "\n",
    "Epoch 6/10\n",
    "Training: 100%|██████████| 500/500 [06:33<00:00,  1.27it/s]\n",
    "Evaluating: 100%|██████████| 125/125 [00:36<00:00,  3.46it/s]\n",
    "Train Loss: 1.7701\n",
    "Val Loss: 2.6317\n",
    "Val Exact Match: 0.6230\n",
    "Val Level 1 Accuracy: 0.9345\n",
    "Val Level 2 Accuracy: 0.8065\n",
    "Val Level 3 Accuracy: 0.6510\n",
    "\n",
    "Epoch 7/10\n",
    "Training: 100%|██████████| 500/500 [06:35<00:00,  1.26it/s]\n",
    "Evaluating: 100%|██████████| 125/125 [00:36<00:00,  3.45it/s]\n",
    "Train Loss: 1.3490\n",
    "Val Loss: 2.4654\n",
    "Val Exact Match: 0.6745\n",
    "Val Level 1 Accuracy: 0.9340\n",
    "Val Level 2 Accuracy: 0.8240\n",
    "Val Level 3 Accuracy: 0.7035\n",
    "\n",
    "Epoch 8/10\n",
    "Training: 100%|██████████| 500/500 [06:35<00:00,  1.26it/s]\n",
    "Evaluating: 100%|██████████| 125/125 [00:34<00:00,  3.57it/s]\n",
    "Train Loss: 1.0143\n",
    "Val Loss: 2.4604\n",
    "Val Exact Match: 0.6985\n",
    "Val Level 1 Accuracy: 0.9345\n",
    "Val Level 2 Accuracy: 0.8285\n",
    "Val Level 3 Accuracy: 0.7195\n",
    "\n",
    "Epoch 9/10\n",
    "Training: 100%|██████████| 500/500 [06:32<00:00,  1.27it/s]\n",
    "Evaluating: 100%|██████████| 125/125 [00:36<00:00,  3.45it/s]\n",
    "Train Loss: 0.7739\n",
    "Val Loss: 2.3518\n",
    "Val Exact Match: 0.7195\n",
    "Val Level 1 Accuracy: 0.9365\n",
    "Val Level 2 Accuracy: 0.8360\n",
    "Val Level 3 Accuracy: 0.7440\n",
    "\n",
    "Epoch 10/10\n",
    "Training: 100%|██████████| 500/500 [06:34<00:00,  1.27it/s]\n",
    "Evaluating: 100%|██████████| 125/125 [00:35<00:00,  3.54it/s]\n",
    "Train Loss: 0.5850\n",
    "Val Loss: 2.3248\n",
    "Val Exact Match: 0.7330\n",
    "Val Level 1 Accuracy: 0.9315\n",
    "Val Level 2 Accuracy: 0.8395\n",
    "Val Level 3 Accuracy: 0.7540\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shallow Learning\n",
    "### Model: Support Vector Machine(SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalSVMClassifier:\n",
    "    def __init__(self):\n",
    "        self.tfidf = TfidfVectorizer(\n",
    "            max_features=10000,\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=2\n",
    "        )\n",
    "        self.level1_clf = LinearSVC(\n",
    "            C=1.0,\n",
    "            class_weight='balanced',\n",
    "            max_iter=1000\n",
    "        )\n",
    "        self.level2_clfs = {}\n",
    "        self.level3_clfs = {}\n",
    "        self.le1 = LabelEncoder()\n",
    "        self.le2 = LabelEncoder()\n",
    "        self.le3 = LabelEncoder()\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean text data\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return ''\n",
    "        text = str(text)\n",
    "\n",
    "        text = text.lower()\n",
    "\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    def prepare_data(self, df):\n",
    "        \"\"\"Prepare text and labels\"\"\"\n",
    "        print(\"Preparing data...\")\n",
    "        \n",
    "        # Clean and combine title and text\n",
    "        print(\"Cleaning text data...\")\n",
    "        df['Title_clean'] = df['Title'].apply(self.clean_text)\n",
    "        df['Text_clean'] = df['Text'].apply(self.clean_text)\n",
    "        df['text_combined'] = df['Title_clean'] + ' ' + df['Text_clean']\n",
    "        \n",
    "        print(\"Removing empty texts...\")\n",
    "        mask = df['text_combined'].str.strip() != ''\n",
    "        df = df[mask].copy()\n",
    "        \n",
    "\n",
    "        print(f\"\\nTotal samples after cleaning: {len(df)}\")\n",
    "        print(f\"Number of unique categories:\")\n",
    "        print(f\"Cat1: {df['Cat1'].nunique()}\")\n",
    "        print(f\"Cat2: {df['Cat2'].nunique()}\")\n",
    "        print(f\"Cat3: {df['Cat3'].nunique()}\")\n",
    "        \n",
    "        # Encode labels\n",
    "        print(\"\\nEncoding labels...\")\n",
    "        df['Cat1_encoded'] = self.le1.fit_transform(df['Cat1'])\n",
    "        df['Cat2_encoded'] = self.le2.fit_transform(df['Cat2'])\n",
    "        df['Cat3_encoded'] = self.le3.fit_transform(df['Cat3'])\n",
    "        \n",
    "\n",
    "        print(\"Creating TF-IDF features...\")\n",
    "        X = self.tfidf.fit_transform(df['text_combined'])\n",
    "        print(f\"Feature matrix shape: {X.shape}\")\n",
    "        \n",
    "        # Split data\n",
    "        print(\"\\nSplitting data...\")\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X,\n",
    "            df[['Cat1_encoded', 'Cat2_encoded', 'Cat3_encoded']],\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            stratify=df['Cat1_encoded']  # Stratify by Cat1 to maintain distribution\n",
    "        )\n",
    "        \n",
    "        print(f\"Training samples: {X_train.shape[0]}\")\n",
    "        print(f\"Testing samples: {X_test.shape[0]}\")\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"Train hierarchical SVM with handling for single-class cases\"\"\"\n",
    "        print(\"\\nTraining Level 1 classifier...\")\n",
    "        self.level1_clf.fit(X_train, y_train['Cat1_encoded'])\n",
    "        \n",
    "        # Train Level 2 classifiers\n",
    "        print(\"\\nTraining Level 2 classifiers...\")\n",
    "        for cat1 in tqdm(y_train['Cat1_encoded'].unique()):\n",
    "            mask = y_train['Cat1_encoded'] == cat1\n",
    "            if mask.sum() > 0:\n",
    "                X_cat = X_train[mask]\n",
    "                y_cat = y_train.loc[mask, 'Cat2_encoded']\n",
    "                \n",
    "                # Check number of unique classes\n",
    "                if len(np.unique(y_cat)) > 1:\n",
    "                    # Train classifier\n",
    "                    clf = LinearSVC(C=1.0, class_weight='balanced', max_iter=1000)\n",
    "                    clf.fit(X_cat, y_cat)\n",
    "                    self.level2_clfs[cat1] = clf\n",
    "                else:\n",
    "                    # Store the single class for direct prediction\n",
    "                    self.level2_clfs[cat1] = np.unique(y_cat)[0]\n",
    "        \n",
    "        # Train Level 3 classifiers\n",
    "        print(\"\\nTraining Level 3 classifiers...\")\n",
    "        skipped_cats = []\n",
    "        for cat2 in tqdm(y_train['Cat2_encoded'].unique()):\n",
    "            mask = y_train['Cat2_encoded'] == cat2\n",
    "            if mask.sum() > 0:\n",
    "                X_cat = X_train[mask]\n",
    "                y_cat = y_train.loc[mask, 'Cat3_encoded']\n",
    "                \n",
    "                if len(np.unique(y_cat)) > 1:\n",
    "\n",
    "                    clf = LinearSVC(C=1.0, class_weight='balanced', max_iter=1000)\n",
    "                    clf.fit(X_cat, y_cat)\n",
    "                    self.level3_clfs[cat2] = clf\n",
    "                else:\n",
    "                    # Store the single class for direct prediction\n",
    "                    self.level3_clfs[cat2] = np.unique(y_cat)[0]\n",
    "                    skipped_cats.append((cat2, len(y_cat)))\n",
    "        \n",
    "        if skipped_cats:\n",
    "            print(\"\\nSkipped training for following categories (single class):\")\n",
    "            for cat, count in skipped_cats:\n",
    "                print(f\"Category {cat}: {count} samples\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make hierarchical predictions with handling for single-class cases\"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        # Level 1 predictions\n",
    "        l1_preds = self.level1_clf.predict(X)\n",
    "        \n",
    "        # Level 2 and 3 predictions\n",
    "        for i, l1_pred in enumerate(l1_preds):\n",
    "            # Level 2 prediction\n",
    "            if l1_pred in self.level2_clfs:\n",
    "                if isinstance(self.level2_clfs[l1_pred], (int, np.integer)):\n",
    "                    l2_pred = self.level2_clfs[l1_pred]\n",
    "                else:\n",
    "                    l2_pred = self.level2_clfs[l1_pred].predict([X[i].toarray()[0]])[0]\n",
    "            else:\n",
    "                l2_pred = -1\n",
    "            \n",
    "            # Level 3 prediction\n",
    "            if l2_pred in self.level3_clfs:\n",
    "                if isinstance(self.level3_clfs[l2_pred], (int, np.integer)):\n",
    "                    # Direct prediction for single-class case\n",
    "                    l3_pred = self.level3_clfs[l2_pred]\n",
    "                else:\n",
    "                    # Normal prediction\n",
    "                    l3_pred = self.level3_clfs[l2_pred].predict([X[i].toarray()[0]])[0]\n",
    "            else:\n",
    "                l3_pred = -1\n",
    "            \n",
    "            predictions.append((l1_pred, l2_pred, l3_pred))\n",
    "        \n",
    "        return np.array(predictions)\n",
    "    \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        \"\"\"Evaluate the model\"\"\"\n",
    "        print(\"\\nEvaluating model...\")\n",
    "        predictions = self.predict(X_test)\n",
    "        \n",
    "        pred_df = pd.DataFrame({\n",
    "            'Cat1_pred': self.le1.inverse_transform(predictions[:, 0]),\n",
    "            'Cat2_pred': self.le2.inverse_transform(predictions[:, 1]),\n",
    "            'Cat3_pred': self.le3.inverse_transform(predictions[:, 2]),\n",
    "            'Cat1_true': self.le1.inverse_transform(y_test['Cat1_encoded']),\n",
    "            'Cat2_true': self.le2.inverse_transform(y_test['Cat2_encoded']),\n",
    "            'Cat3_true': self.le3.inverse_transform(y_test['Cat3_encoded'])\n",
    "        })\n",
    "        \n",
    "        # Evaluate each level\n",
    "        for level in range(3):\n",
    "            print(f\"\\nLevel {level+1} Results:\")\n",
    "            y_true = y_test[f'Cat{level+1}_encoded']\n",
    "            y_pred = predictions[:, level]\n",
    "            \n",
    "            # Calculate metrics\n",
    "            accuracy = accuracy_score(y_true, y_pred)\n",
    "            print(f\"Accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "            print(\"\\nClassification Report:\")\n",
    "            print(classification_report(y_true, y_pred))\n",
    "        \n",
    "        exact_matches = sum(1 for p, t in zip(predictions, y_test.values) if all(p == t))\n",
    "        print(f\"\\nExact Match Ratio: {exact_matches/len(predictions):.4f}\")\n",
    "        \n",
    "        pred_df.to_csv('svm_predictions_with_categories.csv', index=False)\n",
    "        \n",
    "        return predictions, pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Cleaning text data...\n",
      "Removing empty texts...\n",
      "\n",
      "Total samples after cleaning: 10000\n",
      "Number of unique categories:\n",
      "Cat1: 6\n",
      "Cat2: 64\n",
      "Cat3: 234\n",
      "\n",
      "Encoding labels...\n",
      "Creating TF-IDF features...\n",
      "Feature matrix shape: (10000, 10000)\n",
      "\n",
      "Splitting data...\n",
      "Training samples: 8000\n",
      "Testing samples: 2000\n",
      "\n",
      "Training Level 1 classifier...\n",
      "\n",
      "Training Level 2 classifiers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 24.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Level 3 classifiers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 298.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skipped training for following categories (single class):\n",
      "Category 62: 35 samples\n",
      "Category 9: 14 samples\n",
      "Category 16: 21 samples\n",
      "Category 49: 12 samples\n",
      "Category 59: 17 samples\n",
      "Category 29: 21 samples\n",
      "Category 17: 9 samples\n",
      "Category 56: 6 samples\n",
      "Category 41: 3 samples\n",
      "Category 35: 17 samples\n",
      "Category 30: 16 samples\n",
      "Category 50: 6 samples\n",
      "Category 40: 3 samples\n",
      "Category 3: 1 samples\n",
      "\n",
      "Evaluating model...\n",
      "\n",
      "Level 1 Results:\n",
      "Accuracy: 0.8890\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.82      0.85       140\n",
      "           1       0.88      0.92      0.90       427\n",
      "           2       0.85      0.91      0.88       168\n",
      "           3       0.88      0.85      0.87       598\n",
      "           4       0.94      0.93      0.93       315\n",
      "           5       0.90      0.90      0.90       352\n",
      "\n",
      "    accuracy                           0.89      2000\n",
      "   macro avg       0.89      0.89      0.89      2000\n",
      "weighted avg       0.89      0.89      0.89      2000\n",
      "\n",
      "\n",
      "Level 2 Results:\n",
      "Accuracy: 0.7365\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.44      0.46        32\n",
      "           1       0.46      0.38      0.41        16\n",
      "           2       0.86      0.55      0.67        11\n",
      "           3       0.00      0.00      0.00         1\n",
      "           4       0.38      0.55      0.45        20\n",
      "           5       0.86      0.55      0.67        33\n",
      "           6       0.80      0.67      0.73         6\n",
      "           7       0.80      0.90      0.84        39\n",
      "           8       0.50      0.25      0.33         4\n",
      "           9       0.33      0.12      0.18         8\n",
      "          10       0.83      0.71      0.77         7\n",
      "          11       0.67      0.80      0.73        15\n",
      "          12       0.50      0.25      0.33         4\n",
      "          13       0.59      0.81      0.68        36\n",
      "          14       0.50      1.00      0.67         2\n",
      "          15       0.82      0.85      0.83       100\n",
      "          16       0.00      0.00      0.00         4\n",
      "          17       0.50      1.00      0.67         1\n",
      "          18       0.93      0.90      0.92        30\n",
      "          19       0.85      0.86      0.86       167\n",
      "          20       0.71      0.83      0.77        12\n",
      "          21       0.53      0.64      0.58        25\n",
      "          22       0.31      0.17      0.22        24\n",
      "          23       0.68      0.62      0.65        21\n",
      "          24       0.89      0.89      0.89        35\n",
      "          25       0.92      0.94      0.93       117\n",
      "          26       0.00      0.00      0.00         1\n",
      "          27       0.79      0.79      0.79        39\n",
      "          28       0.83      0.87      0.85        23\n",
      "          29       1.00      1.00      1.00         3\n",
      "          30       0.50      0.60      0.55         5\n",
      "          31       0.14      0.08      0.11        12\n",
      "          32       0.85      0.93      0.89       113\n",
      "          33       0.00      0.00      0.00         1\n",
      "          34       0.67      0.61      0.64       140\n",
      "          35       0.25      0.20      0.22         5\n",
      "          36       0.53      0.50      0.51        18\n",
      "          37       0.74      0.73      0.74        67\n",
      "          38       0.29      0.38      0.33        29\n",
      "          39       0.85      0.91      0.88        69\n",
      "          41       0.00      0.00      0.00         2\n",
      "          42       0.81      0.74      0.77        84\n",
      "          43       0.39      0.50      0.44        14\n",
      "          44       0.73      0.64      0.68        25\n",
      "          45       0.79      0.83      0.81       179\n",
      "          46       0.59      0.63      0.61        43\n",
      "          47       0.79      0.80      0.80       101\n",
      "          48       1.00      1.00      1.00         3\n",
      "          49       0.67      1.00      0.80         2\n",
      "          50       0.00      0.00      0.00         1\n",
      "          51       0.68      0.65      0.67        20\n",
      "          52       0.86      0.63      0.73        19\n",
      "          53       0.00      0.00      0.00         0\n",
      "          54       0.80      0.50      0.62        16\n",
      "          55       0.64      0.79      0.71        84\n",
      "          56       0.00      0.00      0.00         5\n",
      "          57       0.60      0.60      0.60        15\n",
      "          58       0.50      0.45      0.48        11\n",
      "          59       1.00      1.00      1.00         5\n",
      "          60       0.68      0.70      0.69        30\n",
      "          61       0.33      0.18      0.24        11\n",
      "          62       0.89      0.50      0.64        16\n",
      "          63       0.56      0.53      0.54        19\n",
      "\n",
      "    accuracy                           0.74      2000\n",
      "   macro avg       0.58      0.57      0.56      2000\n",
      "weighted avg       0.73      0.74      0.73      2000\n",
      "\n",
      "\n",
      "Level 3 Results:\n",
      "Accuracy: 0.6565\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.43      0.50         7\n",
      "           2       1.00      1.00      1.00         3\n",
      "           3       0.83      0.62      0.71         8\n",
      "           4       0.80      0.67      0.73         6\n",
      "           5       0.47      0.50      0.48        16\n",
      "           6       0.67      0.69      0.68        29\n",
      "           7       1.00      1.00      1.00         6\n",
      "           8       0.33      0.75      0.46         4\n",
      "           9       0.00      0.00      0.00         3\n",
      "          10       0.00      0.00      0.00         2\n",
      "          11       0.00      0.00      0.00         3\n",
      "          12       0.00      0.00      0.00         2\n",
      "          13       0.00      0.00      0.00         1\n",
      "          14       0.00      0.00      0.00         3\n",
      "          15       0.71      0.83      0.77         6\n",
      "          16       1.00      0.33      0.50         3\n",
      "          17       1.00      0.20      0.33         5\n",
      "          18       0.00      0.00      0.00         1\n",
      "          19       0.00      0.00      0.00         0\n",
      "          20       1.00      1.00      1.00         4\n",
      "          21       0.80      0.80      0.80         5\n",
      "          23       0.94      0.85      0.89        34\n",
      "          24       0.50      0.67      0.57         3\n",
      "          26       0.00      0.00      0.00         0\n",
      "          27       0.48      0.88      0.62        17\n",
      "          28       0.67      0.64      0.65        28\n",
      "          29       0.50      0.33      0.40         3\n",
      "          30       0.80      0.93      0.86        30\n",
      "          31       0.50      0.33      0.40         3\n",
      "          32       0.33      0.12      0.18         8\n",
      "          33       1.00      0.60      0.75         5\n",
      "          35       0.71      0.83      0.77         6\n",
      "          36       0.50      0.86      0.63         7\n",
      "          38       0.00      0.00      0.00         2\n",
      "          39       0.00      0.00      0.00         2\n",
      "          40       0.29      0.29      0.29         7\n",
      "          41       0.50      0.33      0.40         3\n",
      "          42       1.00      1.00      1.00         2\n",
      "          44       0.80      0.36      0.50        11\n",
      "          45       0.67      1.00      0.80         2\n",
      "          46       0.71      0.71      0.71         7\n",
      "          47       0.25      1.00      0.40         1\n",
      "          48       0.00      0.00      0.00         1\n",
      "          49       0.33      0.50      0.40         2\n",
      "          50       0.50      0.67      0.57         6\n",
      "          51       0.00      0.00      0.00         2\n",
      "          52       0.50      0.33      0.40         3\n",
      "          53       0.60      0.55      0.57        11\n",
      "          54       0.74      0.56      0.64        25\n",
      "          55       0.71      0.83      0.77         6\n",
      "          56       0.68      0.83      0.75        18\n",
      "          57       0.46      0.60      0.52        10\n",
      "          58       0.38      0.68      0.49        22\n",
      "          59       0.00      0.00      0.00         4\n",
      "          60       0.25      0.50      0.33         2\n",
      "          61       0.67      0.29      0.40         7\n",
      "          62       0.33      0.22      0.27         9\n",
      "          63       0.67      0.50      0.57        12\n",
      "          64       0.50      1.00      0.67         1\n",
      "          65       1.00      0.75      0.86         4\n",
      "          66       1.00      0.67      0.80         3\n",
      "          67       0.71      1.00      0.83         5\n",
      "          68       0.00      0.00      0.00         5\n",
      "          69       1.00      1.00      1.00        19\n",
      "          70       0.60      0.75      0.67         4\n",
      "          71       0.44      0.57      0.50         7\n",
      "          72       0.00      0.00      0.00         1\n",
      "          73       0.50      0.33      0.40         6\n",
      "          74       0.40      1.00      0.57         2\n",
      "          76       1.00      0.25      0.40         4\n",
      "          77       0.40      0.29      0.33         7\n",
      "          78       0.12      0.07      0.09        14\n",
      "          79       0.50      0.50      0.50         2\n",
      "          80       0.64      1.00      0.78         7\n",
      "          81       0.66      0.82      0.73        51\n",
      "          82       1.00      1.00      1.00         2\n",
      "          83       0.00      0.00      0.00         1\n",
      "          84       0.82      0.53      0.64        17\n",
      "          85       0.41      0.55      0.47        20\n",
      "          86       0.40      0.25      0.31        16\n",
      "          87       0.00      0.00      0.00         3\n",
      "          88       0.00      0.00      0.00         1\n",
      "          89       0.50      0.17      0.25         6\n",
      "          90       0.64      0.70      0.67        10\n",
      "          91       0.54      0.50      0.52        14\n",
      "          93       0.00      0.00      0.00         1\n",
      "          94       0.64      0.64      0.64        14\n",
      "          95       0.75      0.60      0.67         5\n",
      "          96       1.00      0.17      0.29         6\n",
      "          97       0.83      0.71      0.77        14\n",
      "          99       0.50      0.33      0.40         3\n",
      "         100       1.00      1.00      1.00         3\n",
      "         101       0.50      0.60      0.55         5\n",
      "         102       0.65      0.65      0.65        17\n",
      "         103       0.00      0.00      0.00         6\n",
      "         104       0.00      0.00      0.00         0\n",
      "         105       0.00      0.00      0.00         2\n",
      "         106       0.76      0.93      0.84        14\n",
      "         107       0.60      0.60      0.60         5\n",
      "         108       0.00      0.00      0.00         5\n",
      "         109       0.36      0.57      0.44         7\n",
      "         110       0.52      0.89      0.65        19\n",
      "         112       0.86      0.67      0.75         9\n",
      "         113       0.50      0.22      0.31         9\n",
      "         114       0.76      0.62      0.68        26\n",
      "         115       0.84      0.84      0.84        32\n",
      "         116       0.75      0.21      0.33        14\n",
      "         117       1.00      0.80      0.89         5\n",
      "         118       0.00      0.00      0.00         3\n",
      "         120       1.00      0.33      0.50         3\n",
      "         121       0.93      0.96      0.94        26\n",
      "         122       0.36      0.75      0.49        12\n",
      "         123       0.00      0.00      0.00         3\n",
      "         124       0.86      0.75      0.80         8\n",
      "         125       1.00      0.50      0.67         2\n",
      "         126       0.54      0.58      0.56        12\n",
      "         127       0.80      0.80      0.80         5\n",
      "         128       1.00      1.00      1.00         4\n",
      "         129       0.00      0.00      0.00         4\n",
      "         130       0.50      0.17      0.25         6\n",
      "         131       1.00      0.80      0.89         5\n",
      "         132       1.00      0.57      0.73         7\n",
      "         133       0.67      1.00      0.80         4\n",
      "         134       0.93      1.00      0.96        26\n",
      "         136       1.00      0.50      0.67         2\n",
      "         137       0.20      0.14      0.17         7\n",
      "         138       0.00      0.00      0.00         1\n",
      "         139       0.50      0.47      0.48        15\n",
      "         140       0.67      1.00      0.80         2\n",
      "         142       0.00      0.00      0.00         2\n",
      "         143       0.00      0.00      0.00         3\n",
      "         144       0.84      0.90      0.87        48\n",
      "         145       1.00      0.50      0.67         2\n",
      "         146       0.75      0.67      0.71         9\n",
      "         147       0.20      0.25      0.22         8\n",
      "         148       0.33      0.25      0.29         4\n",
      "         149       0.00      0.00      0.00         2\n",
      "         150       0.94      0.98      0.96        45\n",
      "         151       0.00      0.00      0.00         3\n",
      "         152       0.20      0.33      0.25         6\n",
      "         153       0.00      0.00      0.00         3\n",
      "         154       0.00      0.00      0.00         3\n",
      "         155       0.75      0.69      0.72        26\n",
      "         157       0.92      0.79      0.85        29\n",
      "         158       0.49      0.77      0.60        22\n",
      "         159       0.00      0.00      0.00         2\n",
      "         160       0.00      0.00      0.00         6\n",
      "         161       0.30      0.75      0.43         4\n",
      "         162       0.00      0.00      0.00         2\n",
      "         163       0.50      0.50      0.50         4\n",
      "         164       0.67      0.33      0.44         6\n",
      "         165       0.45      0.62      0.53         8\n",
      "         166       0.33      0.25      0.29         4\n",
      "         167       0.86      0.86      0.86         7\n",
      "         168       0.00      0.00      0.00         7\n",
      "         169       0.67      0.67      0.67         3\n",
      "         170       1.00      0.80      0.89         5\n",
      "         171       1.00      1.00      1.00         3\n",
      "         173       0.53      0.64      0.58        25\n",
      "         174       0.00      0.00      0.00         1\n",
      "         175       0.63      0.80      0.71        15\n",
      "         176       0.60      0.43      0.50         7\n",
      "         177       0.00      0.00      0.00         2\n",
      "         178       0.00      0.00      0.00         1\n",
      "         179       1.00      0.33      0.50         3\n",
      "         180       0.00      0.00      0.00         1\n",
      "         181       0.67      0.40      0.50         5\n",
      "         182       1.00      0.50      0.67         2\n",
      "         183       0.50      0.50      0.50         2\n",
      "         185       0.28      0.47      0.35        19\n",
      "         186       0.89      0.50      0.64        16\n",
      "         187       1.00      0.33      0.50         3\n",
      "         188       0.00      0.00      0.00         3\n",
      "         189       0.82      0.69      0.75        13\n",
      "         190       0.00      0.00      0.00         0\n",
      "         191       0.70      0.82      0.76        57\n",
      "         192       0.75      0.50      0.60         6\n",
      "         193       0.00      0.00      0.00         4\n",
      "         194       0.60      0.75      0.67         4\n",
      "         195       0.00      0.00      0.00         5\n",
      "         196       0.12      0.33      0.18         3\n",
      "         197       0.50      0.50      0.50         6\n",
      "         198       0.25      0.20      0.22         5\n",
      "         199       1.00      1.00      1.00         1\n",
      "         200       0.00      0.00      0.00         1\n",
      "         201       0.67      0.18      0.29        11\n",
      "         202       1.00      0.75      0.86         8\n",
      "         203       0.00      0.00      0.00         1\n",
      "         204       0.00      0.00      0.00         2\n",
      "         205       1.00      0.50      0.67         2\n",
      "         206       0.00      0.00      0.00         4\n",
      "         207       1.00      1.00      1.00         5\n",
      "         208       0.00      0.00      0.00         1\n",
      "         209       0.00      0.00      0.00         0\n",
      "         210       0.77      0.83      0.80        12\n",
      "         211       0.74      0.94      0.83        49\n",
      "         212       1.00      0.25      0.40         4\n",
      "         213       0.80      0.67      0.73         6\n",
      "         214       0.80      1.00      0.89         4\n",
      "         215       0.00      0.00      0.00         3\n",
      "         216       0.82      1.00      0.90        23\n",
      "         217       0.00      0.00      0.00         1\n",
      "         219       0.33      1.00      0.50         1\n",
      "         220       0.77      0.98      0.86        49\n",
      "         221       1.00      0.50      0.67         2\n",
      "         222       0.70      0.58      0.64        12\n",
      "         223       1.00      0.78      0.88         9\n",
      "         224       0.81      0.77      0.79        22\n",
      "         225       0.25      0.17      0.20         6\n",
      "         226       0.00      0.00      0.00         1\n",
      "         227       0.67      0.79      0.73       135\n",
      "         228       1.00      0.67      0.80         3\n",
      "         229       0.50      0.50      0.50         2\n",
      "         230       1.00      0.14      0.25         7\n",
      "         231       1.00      0.33      0.50         3\n",
      "         232       0.88      0.87      0.88        69\n",
      "         233       0.00      0.00      0.00         9\n",
      "\n",
      "    accuracy                           0.66      2000\n",
      "   macro avg       0.51      0.46      0.46      2000\n",
      "weighted avg       0.65      0.66      0.64      2000\n",
      "\n",
      "\n",
      "Exact Match Ratio: 0.6485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nbhagat/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/nbhagat/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/nbhagat/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/nbhagat/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/nbhagat/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/nbhagat/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/nbhagat/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/nbhagat/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/nbhagat/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/nbhagat/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/nbhagat/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/nbhagat/Library/Python/3.9/lib/python/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classifier = HierarchicalSVMClassifier()\n",
    "\n",
    "X_train, X_test, y_train, y_test = classifier.prepare_data(df)\n",
    "classifier.train(X_train, y_train)\n",
    "predictions, pred_df = classifier.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
